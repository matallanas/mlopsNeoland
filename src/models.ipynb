{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps or how not to die to put models in production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLFlow Models\n",
    "In this section, we can see an example of using a trained model from MLFlow experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After decide which model will be used, paste here both experiment_id and run_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = \"<EXPERIMENT_ID>\"\n",
    "run_id = \"<RUN_ID>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "X = np.array(cancer.data)\n",
    "y = np.array(cancer.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, train_size=426, test_size=143, random_state=0)\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = np.random.randint(len(x_test))\n",
    "x_sel = x_test[sel]\n",
    "y_sel = y_test[sel]\n",
    "print(y_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.sklearn.load_model(f'../mlruns/{experiment_id}/{run_id}/artifacts/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([x_sel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a docker image to put in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mlflow models build-docker -m '<MODEL_FILE_PATH>' -n \"<IMAGE_NAME>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run -it -p 6001:8080 \"<IMAGE_NAME>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to request to the microservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def query_endpoint_example(scoring_uri, inputs, service_key=None):\n",
    "  headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "  }\n",
    "  if service_key is not None:\n",
    "    headers[\"Authorization\"] = \"Bearer {service_key}\".format(service_key=service_key)\n",
    "    \n",
    "  print(\"Sending batch prediction request with inputs: {}\".format(inputs))\n",
    "  response = requests.post(scoring_uri, data=json.dumps(inputs), headers=headers)\n",
    "  preds = json.loads(response.text)\n",
    "  print(\"Received response: {}\".format(preds))\n",
    "  return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data for the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_input = {'data': [x_sel.tolist()]}\n",
    "query_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the inference to the microservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_prediction1 = query_endpoint_example(scoring_uri='http://127.0.0.1:6001/invocations', inputs=query_input)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6fc5c6b227698cf9b4a2760cc0480eb1d9d0d041f5e45ec6f31807957ead7b38"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mlflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
